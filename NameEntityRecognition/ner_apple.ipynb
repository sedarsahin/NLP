{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from nltk.chunk import ne_chunk_sents\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Retrieving and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia API to retrieve an article about \"Tesla Inc.\" #\n",
    "user_agent='MyProjectName (merlin@example.com)'\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent=user_agent, language='en')\n",
    "article = wiki_wiki.page('Apple Inc.')\n",
    "# print(article.title)\n",
    "# print(article.summary)\n",
    "\n",
    "# Preprocessing Data ##\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(article.text)\n",
    "print(sentences[:3])\n",
    "\n",
    "# Further tokenize each sentence into words\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "print(token_sentences[0])\n",
    "\n",
    "# # Exercise 1:\n",
    "# # Print the first five sentences\n",
    "# print(sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences = [pos_tag(w) for w in token_sentences]\n",
    "print(pos_sentences[0])\n",
    "\n",
    "# # Exercise 2:\n",
    "# # Print the POS tags for the first ten sentences\n",
    "# for sent in pos_sentences[:10]:\n",
    "#     print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Named Entity Recognition Name Entity Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the named entity chunks\n",
    "chunked_sentences = ne_chunk_sents(pos_sentences)\n",
    "\n",
    "# Create a defaultdict for NER categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Count and categorize named entities\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "\n",
    "\n",
    "# # Exercise 3:\n",
    "# # Identify and print the named entities detected\n",
    "# for sent in chunked_sentences:\n",
    "#     for chunk in sent:\n",
    "#         if hasattr(chunk, 'label'):\n",
    "#             print(chunk)\n",
    "\n",
    "# # Count and display the number of named entities for different categories\n",
    "# print(ner_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualizing NER Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels and values for the pie chart\n",
    "labels = list(ner_categories.keys())\n",
    "values = list(ner_categories.values())\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.show()\n",
    "\n",
    "# # Exercise 4:\n",
    "# # Customize the pie chart\n",
    "# # Display percentages with no decimal points\n",
    "# plt.pie(values, labels=labels, autopct='%1.0f%%', startangle=140)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
